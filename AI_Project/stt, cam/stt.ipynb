{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d654436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#설치\n",
    "#pip install google-cloud-speech\n",
    "#pip install PyAudio-0.2.11-cp39-cp39-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba4666c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캡처\n",
      " 캡처\n",
      " 캡처\n",
      " 캡처\n",
      " 캡처\n",
      "이미지개수 = 2\n",
      "예측한 값은 1입니다.\n",
      "예측한 값은 2입니다.\n",
      "이미지개수 = 1\n",
      "예측한 값은 3입니다.\n",
      "이미지개수 = 2\n",
      "예측한 값은 4입니다.\n",
      "예측한 값은 5입니다.\n",
      "이미지개수 = 1\n",
      "예측한 값은 6입니다.\n",
      "이미지개수 = 1\n",
      "예측한 값은 7입니다.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43960/1352751000.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;31m# [END speech_transcribe_streaming_mic]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43960/1352751000.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;31m# Now, put the transcription responses to use.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mlisten_print_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43960/1352751000.py\u001b[0m in \u001b[0;36mlisten_print_loop\u001b[1;34m(responses)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m#                     img = cv2.imread('fails.jpg')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'capture{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                         \u001b[0msrc_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_OTSU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"speechtotext-345712-c93482c9ad85.json\"\n",
    "\n",
    "\"\"\"Google Cloud Speech API sample application using the streaming API.\n",
    "NOTE: This module requires the additional dependency `pyaudio`. To install\n",
    "using pip:\n",
    "    pip install pyaudio\n",
    "Example usage:\n",
    "    python transcribe_streaming_mic.py\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from google.cloud import speech\n",
    "import pyaudio\n",
    "from six.moves import queue\n",
    "import numpy as np\n",
    "from os import remove\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms\n",
    "# tt파일 만들고 시작\n",
    "li_num_zero = ['10']\n",
    "with open('tt.txt','w',encoding='UTF-8') as f:\n",
    "    for name in li_num_zero:\n",
    "        f.write(name+'\\n')\n",
    "\n",
    "li = ['0']\n",
    "with open('division_name.txt','w',encoding='UTF-8') as f:\n",
    "    for name in li:\n",
    "        f.write(name+'\\n')\n",
    "\n",
    "class MicrophoneStream(object):\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "    def __init__(self, rate, chunk):\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1, rate=self._rate,\n",
    "            input=True, frames_per_buffer=self._chunk,\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "            yield b''.join(data)\n",
    "def classification_number(img):\n",
    "    load_model = keras.models.load_model('./num_class.h5')\n",
    "    img = cv2.resize(img, (32,32), interpolation = cv2.INTER_AREA)\n",
    "    pred = load_model.predict(img.reshape(1, 32, 32, 3))\n",
    "    pred = list(pred[0])\n",
    "    result = max(pred)\n",
    "    if result > 0.9:\n",
    "        return pred.index(result)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def listen_print_loop(responses):\n",
    "    num_chars_printed = 0\n",
    "    cur_page = 0\n",
    "    #저장되는 캡쳐개수\n",
    "    count = 0\n",
    "    #저장된 캡쳐개수에 따라 1씩증가\n",
    "    num = 1\n",
    "    #진행하는 전체 개수\n",
    "    cheak = 1\n",
    "    li = []\n",
    "    #종이검출 후 인식되는 숫자개수\n",
    "    num_count = 0\n",
    "    while cv2.waitKey(33) < 0:\n",
    "        for response in responses:\n",
    "            result = response.results[0]\n",
    "            transcript = result.alternatives[0].transcript\n",
    "            overwrite_chars = ' ' * (num_chars_printed - len(transcript))\n",
    "\n",
    "            if not result.is_final:\n",
    "                sys.stdout.write(transcript + overwrite_chars + '\\r')\n",
    "                sys.stdout.flush()\n",
    "                if  '다음' in transcript + overwrite_chars:\n",
    "                    print('다음 설명서 출력')\n",
    "                    cur_page += 1\n",
    "                    with open('./tt.txt', 'w') as f:\n",
    "                        f.write(str(cur_page))\n",
    "                    \n",
    "                if '이전' in transcript + overwrite_chars:\n",
    "                    print('이전 설명서 출력')\n",
    "                    cur_page -= 1\n",
    "                    with open('./tt.txt', 'w') as f:\n",
    "                        f.write(str(cur_page))\n",
    "                        # 현재 화면 캡쳐jpg\", frame)\n",
    "                    # 저장되는 캡쳐 개수\n",
    "                if '캡처' in transcript + overwrite_chars:\n",
    "                    capture = cv2.VideoCapture(0)\n",
    "                    ret, frame = capture.read()\n",
    "                    cv2.imwrite('capture2{}'.format(count) + \".jpg\",frame)\n",
    "                    count += 1\n",
    "                    capture.release()\n",
    "                    \n",
    "                    # 캡쳐된 이미지에 대해 이미지 사이즈 조절 + 분할 + 분할된 이미지 파일명 txt파일로 저장\n",
    "                if '시작' in transcript + overwrite_chars:\n",
    "                    # 저장된 캡쳐 개수와 분할하기위해 읽어온 개수가 같으면 stop\n",
    "                    def reorderPts(pts): # 꼭지점 순서 정렬\n",
    "                        idx = np.lexsort((pts[:, 1], pts[:, 0]))  # 칼럼0 -> 칼럼1 순으로 정렬한 인덱스를 반환\n",
    "                        pts = pts[idx]  # x좌표로 정렬\n",
    "\n",
    "                        if pts[0, 1] > pts[1, 1]:\n",
    "                            pts[[0, 1]] = pts[[1, 0]] # 스와핑\n",
    "\n",
    "                        if pts[2, 1] < pts[3, 1]:\n",
    "                            pts[[2, 3]] = pts[[3, 2]] # 스와핑\n",
    "\n",
    "                        return pts \n",
    "                    while num <= count:\n",
    "                        # output 세로, 가로 크기\n",
    "                        dw, dh = 1080, 1440\n",
    "                        srcQuad = np.array([[0, 0], [0, 0], [0, 0], [0, 0]], np.float32)\n",
    "                        dstQuad = np.array([[0, 0], [0, dh], [dw, dh], [dw, 0]], np.float32)\n",
    "    #                     img = cv2.imread('fails.jpg')\n",
    "                        img = cv2.imread('capture{}'.format(num) + \".jpg\")\n",
    "                        src_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                        _, src_bin = cv2.threshold(src_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "                        # 외곽선 검출 및 명함 검출\n",
    "                        contours, _ = cv2.findContours(src_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "                        cpy = img.copy()\n",
    "                        for pts in contours:\n",
    "                            # 너무 작은 객체는 무시\n",
    "                            if cv2.contourArea(pts) < 1000:\n",
    "                                continue\n",
    "\n",
    "                            # 외곽선 근사화\n",
    "                            approx = cv2.approxPolyDP(pts, cv2.arcLength(pts, True)*0.02, True)\n",
    "\n",
    "                            # 컨벡스가 아니고, 사각형이 아니면 무시\n",
    "                            if not cv2.isContourConvex(approx) or len(approx) != 4:\n",
    "                                continue\n",
    "\n",
    "                            cv2.polylines(cpy, [approx], True, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                            srcQuad = reorderPts(approx.reshape(4, 2).astype(np.float32))\n",
    "\n",
    "                        pers = cv2.getPerspectiveTransform(srcQuad, dstQuad)\n",
    "                        dst = cv2.warpPerspective(img, pers, (dw, dh))\n",
    "                    # ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ검출개수 확인ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "                        imgray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "                        rgbs = imgray.copy()\n",
    "\n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "                        grad = cv2.morphologyEx(rgbs, cv2.MORPH_GRADIENT, kernel)\n",
    "                        _, bw = cv2.threshold(grad, 0.0, 255.0, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "                        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 1))\n",
    "                        connected = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "                        contours, hierarchy = cv2.findContours(connected.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "                        mask = np.zeros(bw.shape, dtype=np.uint8)\n",
    "\n",
    "                        tmps = []\n",
    "                        for idx in range(len(contours)):\n",
    "                            x, y, w, h = cv2.boundingRect(contours[idx])\n",
    "                            mask[y:y+h, x:x+w] = 0\n",
    "                            cv2.drawContours(mask, contours, idx, (255, 255, 255), -1)\n",
    "                            r = float(cv2.countNonZero(mask[y:y+h, x:x+w])) / (w * h)\n",
    "                            if r > 0.45 and w > 8 and w < 50 and h > 8:\n",
    "                                cv2.rectangle(rgbs, (x, y), (x+w-1, y+h-1), (0, 255, 0), 2)\n",
    "                                tmps.append(dst[y:y+h-1, x:x+w-1,:])\n",
    "                        # 50, 즉 흐릿한 이미지에대해 순서 이미지가 아니라고 판단하여 이진화 진행\n",
    "                        for i in tmps:\n",
    "                            ret, px = cv2.threshold(i, 50, 255, cv2.THRESH_BINARY)\n",
    "                            # 다차원 배열을 for문을 통해 하나의 int값으로 받아와 0의개수 즉 \n",
    "                            # 검정색에 해당하는 개수가 500개 이상이면 순서에 해당하는 번호라고 판단하여 count\n",
    "                            num_c = 0\n",
    "                            for i in px:\n",
    "                                for j in i:\n",
    "                                    for k in j:\n",
    "                                        if int(k) == 0:\n",
    "                                            num_c+=1\n",
    "                            if num_c >= 300:\n",
    "                                num_count += 1\n",
    "                    # ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ분할ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "                        # 이미지 분할\n",
    "                        imgray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "                        img2=imgray.copy()\n",
    "                        #이미지 이진화\n",
    "                        blur = cv2.GaussianBlur(imgray, (3,3), 0) \n",
    "                        thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "                        # Morph operations\n",
    "                        # 외곽선\n",
    "                        edge = cv2.Canny(thresh, 100, 200) \n",
    "                        # count한 개수가 1개 이하면 세로축을 크게잡아 하나의 화면에 출력\n",
    "                        if num_count <= 1:\n",
    "                            kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1000,400))\n",
    "                            num_count = 0\n",
    "                            print('이미지개수 = {}'.format(1))\n",
    "                        # count한 개수가 2개 이상이면 세로축을 작게 잡아 여러개로 분할\n",
    "                        elif num_count > 1:\n",
    "                            kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(500,50))\n",
    "                            num_count = 0\n",
    "                            print('이미지개수 = {}'.format(2))\n",
    "                        closed = cv2.morphologyEx(edge, cv2.MORPH_CLOSE, kernel) \n",
    "\n",
    "                        #contours가 찾은 경계의 배열 \n",
    "                        contours, hierarchy = cv2.findContours(closed.copy(),cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) \n",
    "                        contours_xy = np.array(contours , dtype=object) \n",
    "                        contours_xy.shape\n",
    "\n",
    "                        #한페이지 내에서 이미지 순서대로 불러오기 \n",
    "                        contours=reversed(contours)\n",
    "                        #한페이지 내의 모든 폐곡선 범위에 대해 실행 \n",
    "                        top=[]\n",
    "\n",
    "                        for c in contours:\n",
    "                            x,y,w,h = cv2.boundingRect(c)\n",
    "                            top.append(y) \n",
    "                            total=len(top)-1\n",
    "                            #동적 변수 생성으로 순서대로 이미지 입력\n",
    "                            globals()['division_img{}'.format(cheak)] = imgray[y:y+h , x:x+w]\n",
    "                            img = globals()['division_img{}'.format(cheak)]\n",
    "\n",
    "                            if img.shape[0] > 350 and img.shape[1] > 1000:\n",
    "                    #             cv2.imwrite('division_img{}'.format(cheak)+'.png', globals()['division_img{}'.format(cheak)])\n",
    "                                img = img.reshape(img.shape[0], img.shape[1], 1)\n",
    "                                rgb = img.copy()\n",
    "                                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "                                grad = cv2.morphologyEx(rgb, cv2.MORPH_GRADIENT, kernel)\n",
    "                                _, bw = cv2.threshold(grad, 0.0, 255.0, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "                                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 1))\n",
    "                                connected = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "                                contours, hierarchy = cv2.findContours(connected.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "                                mask = np.zeros(bw.shape, dtype=np.uint8)\n",
    "\n",
    "                                tmp = []\n",
    "                                test = []\n",
    "                                for idx in range(len(contours)):\n",
    "                                    x, y, w, h = cv2.boundingRect(contours[idx])\n",
    "                                    mask[y:y+h, x:x+w] = 0\n",
    "                                    cv2.drawContours(mask, contours, idx, (255, 255, 255), -1)\n",
    "                                    r = float(cv2.countNonZero(mask[y:y+h, x:x+w])) / (w * h)\n",
    "                                    if r > 0.45 and w > 8 and w < 50 and h > 8:\n",
    "                                        cv2.rectangle(rgb, (x, y), (x+w-1, y+h-1), (0, 255, 0), 2)\n",
    "                                        # 실행하는 숫자의 크기가 50,50보다 작으므로 해당 조건을 통해 의미없는 이미지 제거\n",
    "                                        test = img[y:y+h-1, x:x+w-1]\n",
    "                                        if test.shape[0] <=50 and test.shape[1] <=50:\n",
    "                                            tmp.append(img[y:y+h-1, x:x+w-1])\n",
    "                                # 좌측 상단에 항상 순서 번호가 검출되기때문에 tmp[-1]에 대해 분류\n",
    "                                tmp[-1] = cv2.cvtColor(tmp[-1], cv2.COLOR_GRAY2BGR)\n",
    "                                print('예측한 값은 {}입니다.'.format(classification_number(tmp[-1])))\n",
    "                                #모델이 인식한 숫자에따라 division_img?.png로 저장\n",
    "                                cv2.imwrite('division_img{}'.format(classification_number(tmp[-1]))+'.jpg', globals()['division_img{}'.format(cheak)])\n",
    "                                #li라는 리스트에 분할된 이미지 이름 저장\n",
    "                                li.append('division_img{}'.format(classification_number(tmp[-1]))+'.jpg')\n",
    "                            cheak += 1\n",
    "\n",
    "                        num += 1\n",
    "                    # division_name 라는 txt파일로 cam과 연계\n",
    "                    with open('division_name.txt','w',encoding='UTF-8') as f:\n",
    "                        for name in li:\n",
    "                            f.write(name+'\\n')\n",
    "                    # tt.txt 숫자 0이 입력되며 시작\n",
    "                    li_num_zero = ['0']\n",
    "                    with open('tt.txt','w',encoding='UTF-8') as f:\n",
    "                        for name in li_num_zero:\n",
    "                            f.write(name+'\\n')\n",
    "                num_chars_printed = len(transcript)\n",
    "            else:\n",
    "                print(transcript + overwrite_chars)\n",
    "\n",
    "                # Exit recognition if any of the transcribed phrases could be\n",
    "                # one of our keywords.\n",
    "                if re.search(r'\\b(exit|quit)\\b', transcript, re.I):\n",
    "                    print('Exiting..')\n",
    "                    break\n",
    "\n",
    "                num_chars_printed = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    language_code = 'ko-KR'  # a BCP-47 language tag\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code)\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config,\n",
    "        interim_results=True)\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (speech.StreamingRecognizeRequest(audio_content=content)\n",
    "                    for content in audio_generator)\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Now, put the transcription responses to use.\n",
    "        listen_print_loop(responses)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "# [END speech_transcribe_streaming_mic]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f930f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
